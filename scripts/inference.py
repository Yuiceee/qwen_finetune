"""
æ¨ç†æµ‹è¯•è„šæœ¬
åŠ è½½LoRAå¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œé‚®ä»¶äº‹ä»¶ä¿¡æ¯æå–
æ”¯æŒå¯¹æ¯”åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹çš„è¾“å‡ºç»“æœ
"""
import torch
import time
import json
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import argparse
from typing import Dict, Tuple

# è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•
if 'HF_HOME' not in os.environ:
    os.environ['HF_HOME'] = '/macroverse/public/database/huggingface/hub'
if 'TRANSFORMERS_CACHE' not in os.environ:
    os.environ['TRANSFORMERS_CACHE'] = '/macroverse/public/database/huggingface/hub'


def load_model(base_model_name, lora_model_path=None):
    """
    åŠ è½½æ¨¡å‹å’Œtokenizer

    Args:
        base_model_name: åŸºç¡€æ¨¡å‹åç§°
        lora_model_path: LoRAæ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneåˆ™åªåŠ è½½åŸºç¡€æ¨¡å‹ï¼‰
    """
    print(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_name}")
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_name,
        trust_remote_code=True
    )

    model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )

    # å¦‚æœæä¾›äº†LoRAæ¨¡å‹è·¯å¾„ï¼Œåˆ™åŠ è½½LoRAæƒé‡
    if lora_model_path:
        print(f"åŠ è½½LoRAæƒé‡: {lora_model_path}")
        model = PeftModel.from_pretrained(model, lora_model_path)
        model = model.merge_and_unload()  # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹

    model.eval()
    return model, tokenizer


def extract_event_info(email_content, model, tokenizer, max_new_tokens=512):
    """
    ä»é‚®ä»¶ä¸­æå–äº‹ä»¶ä¿¡æ¯

    Args:
        email_content: é‚®ä»¶å†…å®¹
        model: æ¨¡å‹
        tokenizer: tokenizer
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°

    Returns:
        response: æå–çš„äº‹ä»¶ä¿¡æ¯
        inference_time: æ¨ç†æ—¶é—´ï¼ˆç§’ï¼‰
    """
    start_time = time.time()

    # æ„å»ºå¯¹è¯æ¶ˆæ¯
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‚®ä»¶äº‹ä»¶ä¿¡æ¯æå–åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": f"è¯·ä»ä»¥ä¸‹é‚®ä»¶ä¸­æå–äº‹ä»¶ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ ‡é¢˜ã€æ—¶é—´ã€åœ°ç‚¹ã€å‚ä¸è€…ç­‰å…³é”®ä¿¡æ¯ï¼Œä»¥JSONæ ¼å¼è¾“å‡ºã€‚\n\né‚®ä»¶å†…å®¹ï¼š\n{email_content}"}
    ]

    # ä½¿ç”¨chat templateæ ¼å¼åŒ–
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    # Tokenize
    inputs = tokenizer(text, return_tensors="pt").to(model.device)

    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,  # å¯ç”¨é‡‡æ ·ä»¥ä½¿ç”¨temperatureå’Œtop_p
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    # è§£ç è¾“å‡º
    response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)

    inference_time = time.time() - start_time

    return response, inference_time


def compare_outputs(base_output: str, finetuned_output: str, ground_truth: str = None) -> Dict:
    """
    å¯¹æ¯”åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹çš„è¾“å‡º

    Args:
        base_output: åŸºç¡€æ¨¡å‹çš„è¾“å‡º
        finetuned_output: å¾®è°ƒæ¨¡å‹çš„è¾“å‡º
        ground_truth: çœŸå®æ ‡æ³¨ï¼ˆå¯é€‰ï¼‰

    Returns:
        å¯¹æ¯”ç»“æœå­—å…¸
    """
    result = {
        'base_valid_json': False,
        'finetuned_valid_json': False,
        'base_parsed': None,
        'finetuned_parsed': None,
        'ground_truth_parsed': None,
        'differences': []
    }

    # è§£æåŸºç¡€æ¨¡å‹è¾“å‡º
    try:
        result['base_parsed'] = json.loads(base_output)
        result['base_valid_json'] = True
    except json.JSONDecodeError:
        result['base_parsed'] = None

    # è§£æå¾®è°ƒæ¨¡å‹è¾“å‡º
    try:
        result['finetuned_parsed'] = json.loads(finetuned_output)
        result['finetuned_valid_json'] = True
    except json.JSONDecodeError:
        result['finetuned_parsed'] = None

    # è§£æçœŸå®æ ‡æ³¨
    if ground_truth:
        try:
            result['ground_truth_parsed'] = json.loads(ground_truth)
        except json.JSONDecodeError:
            result['ground_truth_parsed'] = None

    # å¦‚æœä¸¤ä¸ªæ¨¡å‹éƒ½æˆåŠŸè§£æï¼Œå¯¹æ¯”å­—æ®µå·®å¼‚
    if result['base_parsed'] and result['finetuned_parsed']:
        base_dict = result['base_parsed']
        ft_dict = result['finetuned_parsed']

        all_keys = set(base_dict.keys()) | set(ft_dict.keys())
        for key in all_keys:
            base_val = base_dict.get(key, '(ç¼ºå¤±)')
            ft_val = ft_dict.get(key, '(ç¼ºå¤±)')

            if base_val != ft_val:
                diff_item = {
                    'field': key,
                    'base_value': base_val,
                    'finetuned_value': ft_val
                }

                # å¦‚æœæœ‰çœŸå®æ ‡æ³¨ï¼Œæ·»åŠ å¯¹æ¯”
                if result['ground_truth_parsed'] and key in result['ground_truth_parsed']:
                    diff_item['ground_truth'] = result['ground_truth_parsed'][key]
                    diff_item['base_correct'] = base_val == result['ground_truth_parsed'][key]
                    diff_item['finetuned_correct'] = ft_val == result['ground_truth_parsed'][key]

                result['differences'].append(diff_item)

    return result


def print_comparison(comparison: Dict, base_time: float, ft_time: float):
    """
    æ‰“å°å¯¹æ¯”ç»“æœ

    Args:
        comparison: å¯¹æ¯”ç»“æœå­—å…¸
        base_time: åŸºç¡€æ¨¡å‹æ¨ç†æ—¶é—´
        ft_time: å¾®è°ƒæ¨¡å‹æ¨ç†æ—¶é—´
    """
    print("\n" + "=" * 80)
    print("å¯¹æ¯”ç»“æœ")
    print("=" * 80)

    # JSONæ ¼å¼æ­£ç¡®æ€§
    print(f"\nJSONæ ¼å¼:")
    print(f"  åŸºç¡€æ¨¡å‹: {'âœ“ æ­£ç¡®' if comparison['base_valid_json'] else 'âœ— é”™è¯¯'}")
    print(f"  å¾®è°ƒæ¨¡å‹: {'âœ“ æ­£ç¡®' if comparison['finetuned_valid_json'] else 'âœ— é”™è¯¯'}")

    # æ¨ç†æ—¶é—´
    print(f"\næ¨ç†æ—¶é—´:")
    print(f"  åŸºç¡€æ¨¡å‹: {base_time:.2f}ç§’")
    print(f"  å¾®è°ƒæ¨¡å‹: {ft_time:.2f}ç§’")
    print(f"  é€Ÿåº¦å˜åŒ–: {((ft_time - base_time) / base_time * 100):+.1f}%")

    # å­—æ®µå·®å¼‚
    if comparison['differences']:
        print(f"\nå­—æ®µå·®å¼‚ ({len(comparison['differences'])}ä¸ª):")
        print("-" * 80)
        for diff in comparison['differences']:
            print(f"\n  å­—æ®µ: {diff['field']}")
            print(f"    åŸºç¡€æ¨¡å‹: {diff['base_value']}")
            print(f"    å¾®è°ƒæ¨¡å‹: {diff['finetuned_value']}")

            if 'ground_truth' in diff:
                print(f"    çœŸå®æ ‡æ³¨: {diff['ground_truth']}")
                print(f"    åŸºç¡€æ¨¡å‹å‡†ç¡®: {'âœ“' if diff['base_correct'] else 'âœ—'}")
                print(f"    å¾®è°ƒæ¨¡å‹å‡†ç¡®: {'âœ“' if diff['finetuned_correct'] else 'âœ—'}")
    else:
        print("\nå­—æ®µå·®å¼‚: æ— å·®å¼‚ï¼ˆè¾“å‡ºç›¸åŒï¼‰")

    print("\n" + "=" * 80)


def main(args):
    print("=" * 50)
    print("é‚®ä»¶äº‹ä»¶ä¿¡æ¯æå–æ¨ç†")
    print("=" * 50)

    # å¯¹æ¯”æ¨¡å¼ï¼šåŒæ—¶åŠ è½½åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹
    if args.compare:
        print("\nå¯¹æ¯”æ¨¡å¼ï¼šåŒæ—¶åŠ è½½åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹")
        print(f"åŸºç¡€æ¨¡å‹: {args.base_model}")
        print(f"å¾®è°ƒæ¨¡å‹: {args.lora_model}")

        # åŠ è½½åŸºç¡€æ¨¡å‹
        print("\n[1/2] åŠ è½½åŸºç¡€æ¨¡å‹...")
        base_model, base_tokenizer = load_model(args.base_model, lora_model_path=None)

        # åŠ è½½å¾®è°ƒæ¨¡å‹
        print("\n[2/2] åŠ è½½å¾®è°ƒæ¨¡å‹...")
        ft_model, ft_tokenizer = load_model(args.base_model, args.lora_model)

        print("\nâœ“ ä¸¤ä¸ªæ¨¡å‹åŠ è½½å®Œæˆï¼")
        print("=" * 50)
    else:
        # å•æ¨¡å‹æ¨¡å¼
        model, tokenizer = load_model(args.base_model, args.lora_model)
        print("\nâœ“ æ¨¡å‹åŠ è½½å®Œæˆï¼")
        print("=" * 50)

    # å¦‚æœæä¾›äº†æµ‹è¯•é‚®ä»¶æ–‡ä»¶ï¼Œåˆ™æ‰¹é‡æµ‹è¯•
    if args.test_file:
        print(f"\nä»æ–‡ä»¶è¯»å–æµ‹è¯•é‚®ä»¶: {args.test_file}")
        with open(args.test_file, 'r', encoding='utf-8') as f:
            test_data = [json.loads(line) for line in f]

        print(f"å…± {len(test_data)} æ¡æµ‹è¯•æ ·æœ¬\n")

        # å¯¹æ¯”æ¨¡å¼
        if args.compare:
            for i, item in enumerate(test_data[:args.max_samples] if args.max_samples else test_data):
                print(f"\n{'=' * 80}")
                print(f"æµ‹è¯•æ ·æœ¬ {i + 1}/{min(len(test_data), args.max_samples or len(test_data))}")
                print(f"{'=' * 80}")

                # æ”¯æŒä¸åŒæ•°æ®æ ¼å¼
                if 'messages' in item:
                    email_content = item['messages'][1]['content'].split('é‚®ä»¶å†…å®¹ï¼š\n')[-1]
                    expected_output = item['messages'][2]['content']
                elif 'input' in item:
                    email_content = item['input']
                    expected_output = item.get('output')
                else:
                    print("âš ï¸  æ•°æ®æ ¼å¼ä¸æ”¯æŒï¼Œè·³è¿‡")
                    continue

                print(f"\nğŸ“§ è¾“å…¥é‚®ä»¶:")
                print(f"{email_content[:300]}..." if len(email_content) > 300 else email_content)

                # åŸºç¡€æ¨¡å‹æ¨ç†
                print(f"\n[1/2] åŸºç¡€æ¨¡å‹æ¨ç†ä¸­...")
                base_result, base_time = extract_event_info(email_content, base_model, base_tokenizer, args.max_new_tokens)

                # å¾®è°ƒæ¨¡å‹æ¨ç†
                print(f"[2/2] å¾®è°ƒæ¨¡å‹æ¨ç†ä¸­...")
                ft_result, ft_time = extract_event_info(email_content, ft_model, ft_tokenizer, args.max_new_tokens)

                # æ˜¾ç¤ºåŸå§‹è¾“å‡º
                if args.verbose:
                    print(f"\nğŸ“„ åŸºç¡€æ¨¡å‹è¾“å‡º:\n{base_result}")
                    print(f"\nğŸ“„ å¾®è°ƒæ¨¡å‹è¾“å‡º:\n{ft_result}")
                    if expected_output:
                        print(f"\nğŸ“„ çœŸå®æ ‡æ³¨:\n{expected_output}")

                # å¯¹æ¯”è¾“å‡º
                comparison = compare_outputs(base_result, ft_result, expected_output)
                print_comparison(comparison, base_time, ft_time)

        # å•æ¨¡å‹æ¨¡å¼
        else:
            total_inference_time = 0
            valid_json_count = 0

            for i, item in enumerate(test_data[:args.max_samples] if args.max_samples else test_data):
                print(f"\n{'=' * 50}")
                print(f"æµ‹è¯•æ ·æœ¬ {i + 1}/{min(len(test_data), args.max_samples or len(test_data))}")
                print(f"{'=' * 50}")

                # æ”¯æŒä¸åŒæ•°æ®æ ¼å¼
                if 'messages' in item:
                    email_content = item['messages'][1]['content'].split('é‚®ä»¶å†…å®¹ï¼š\n')[-1]
                    expected_output = item['messages'][2]['content']
                elif 'input' in item:
                    email_content = item['input']
                    expected_output = item['output']
                else:
                    print("âš ï¸  æ•°æ®æ ¼å¼ä¸æ”¯æŒï¼Œè·³è¿‡")
                    continue

                print(f"\nè¾“å…¥é‚®ä»¶ï¼š\n{email_content[:200]}..." if len(email_content) > 200 else f"\nè¾“å…¥é‚®ä»¶ï¼š\n{email_content}")

                result, inference_time = extract_event_info(email_content, model, tokenizer, args.max_new_tokens)
                total_inference_time += inference_time

                print(f"\næ¨¡å‹è¾“å‡ºï¼š\n{result}")
                print(f"\næ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")

                # éªŒè¯JSONæ ¼å¼
                try:
                    json.loads(result)
                    valid_json_count += 1
                    print("âœ“ JSONæ ¼å¼æ­£ç¡®")
                except json.JSONDecodeError:
                    print("âœ— JSONæ ¼å¼é”™è¯¯")

            # ç»Ÿè®¡ä¿¡æ¯
            avg_time = total_inference_time / len(test_data[:args.max_samples] if args.max_samples else test_data)
            json_accuracy = valid_json_count / len(test_data[:args.max_samples] if args.max_samples else test_data) * 100

            print(f"\n{'=' * 50}")
            print("æ¨ç†ç»Ÿè®¡:")
            print(f"{'=' * 50}")
            print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f}ç§’/æ ·æœ¬")
            print(f"JSONæ ¼å¼æ­£ç¡®ç‡: {json_accuracy:.1f}%")
            print(f"{'=' * 50}")

    # äº¤äº’å¼æµ‹è¯•
    elif args.interactive:
        print("\nè¿›å…¥äº¤äº’å¼æµ‹è¯•æ¨¡å¼ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
        print("=" * 50)

        while True:
            print("\nè¯·è¾“å…¥é‚®ä»¶å†…å®¹ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰ï¼š")
            lines = []
            while True:
                line = input()
                if line.strip().lower() == 'quit':
                    print("é€€å‡ºç¨‹åº")
                    return
                if line == "":  # ç©ºè¡Œè¡¨ç¤ºè¾“å…¥ç»“æŸ
                    break
                lines.append(line)

            if not lines:
                continue

            email_content = '\n'.join(lines)

            print("\næå–ä¸­...")
            result, inference_time = extract_event_info(email_content, model, tokenizer, args.max_new_tokens)

            print(f"\næå–ç»“æœï¼š\n{result}")
            print(f"æ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")
            print("=" * 50)

    # å•ä¸ªæµ‹è¯•æ ·ä¾‹
    else:
        test_email = """ä¸»é¢˜ï¼šé¡¹ç›®è¯„å®¡ä¼šè®®
å‘ä»¶äººï¼šé¡¹ç›®ç»ç† å¼ ä¸‰
æ”¶ä»¶äººï¼šå¼€å‘å›¢é˜Ÿ

å„ä½åŒäº‹ï¼Œ

å®šäºæœ¬å‘¨äº”ï¼ˆ12æœˆ29æ—¥ï¼‰ä¸‹åˆ3ç‚¹åœ¨ä¼šè®®å®¤Bå¬å¼€é¡¹ç›®ä¸­æœŸè¯„å®¡ä¼šè®®ã€‚è¯·æŠ€æœ¯è´Ÿè´£äººå’Œæ¶æ„å¸ˆåŠ¡å¿…å‚åŠ ï¼Œå¹¶å‡†å¤‡é¡¹ç›®è¿›å±•æ±‡æŠ¥ææ–™ã€‚

è°¢è°¢ï¼
å¼ ä¸‰"""

        print(f"\næµ‹è¯•é‚®ä»¶ï¼š\n{test_email}")
        print("\næå–ä¸­...")

        result, inference_time = extract_event_info(test_email, model, tokenizer, args.max_new_tokens)

        print(f"\næå–ç»“æœï¼š\n{result}")
        print(f"æ¨ç†æ—¶é—´: {inference_time:.2f}ç§’")
        print("=" * 50)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="é‚®ä»¶äº‹ä»¶ä¿¡æ¯æå–æ¨ç†è„šæœ¬")

    parser.add_argument("--base_model", type=str, default="Qwen/Qwen2.5-7B-Instruct",
                        help="åŸºç¡€æ¨¡å‹åç§°")
    parser.add_argument("--lora_model", type=str, default=None,
                        help="LoRAæ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--test_file", type=str, default=None,
                        help="æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæ ¼å¼ï¼‰")
    parser.add_argument("--max_samples", type=int, default=None,
                        help="æµ‹è¯•æ ·æœ¬æœ€å¤§æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰")
    parser.add_argument("--interactive", action="store_true",
                        help="å¯ç”¨äº¤äº’å¼æµ‹è¯•æ¨¡å¼")
    parser.add_argument("--max_new_tokens", type=int, default=512,
                        help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--compare", action="store_true",
                        help="å¯¹æ¯”åŸºç¡€æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹çš„è¾“å‡ºï¼ˆéœ€è¦åŒæ—¶æŒ‡å®š--lora_modelï¼‰")
    parser.add_argument("--verbose", action="store_true",
                        help="åœ¨å¯¹æ¯”æ¨¡å¼ä¸‹æ˜¾ç¤ºå®Œæ•´çš„æ¨¡å‹è¾“å‡º")

    args = parser.parse_args()

    # éªŒè¯å‚æ•°
    if args.compare and not args.lora_model:
        parser.error("--compare æ¨¡å¼éœ€è¦æŒ‡å®š --lora_model")

    main(args)
