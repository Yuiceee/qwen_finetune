{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 邮件事件信息提取推理\n",
    "\n",
    "这个 notebook 用于加载 LoRA 微调后的模型进行邮件事件信息提取，支持对比基础模型和微调模型的输出结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/macroverse/public/dingsz/infer_optim/qwen_finetune/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/macroverse/public/database/huggingface/hub'\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from typing import Dict, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 配置参数\n",
    "\n",
    "在这里修改你的模型路径和推理参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基础模型: Qwen/Qwen2.5-7B-Instruct\n",
      "LoRA模型: /macroverse/public/dingsz/infer_optim/qwen_finetune/outputs/lora_model/final_model\n"
     ]
    }
   ],
   "source": [
    "# 模型配置\n",
    "BASE_MODEL = \"Qwen/Qwen2.5-7B-Instruct\"  # 基础模型名称\n",
    "LORA_MODEL = \"/macroverse/public/dingsz/infer_optim/qwen_finetune/outputs/lora_model/final_model\"  # LoRA模型路径，例如: \"./output/checkpoint-100\"\n",
    "\n",
    "# 推理参数\n",
    "MAX_NEW_TOKENS = 512  # 最大生成token数\n",
    "TEMPERATURE = 0.7     # 温度参数\n",
    "TOP_P = 0.9          # top_p采样参数\n",
    "\n",
    "# 测试数据配置\n",
    "TEST_FILE = None      # 测试数据文件路径（JSONL格式），例如: \"../data/test.jsonl\"\n",
    "MAX_SAMPLES = 5       # 测试样本最大数量（用于快速测试）\n",
    "\n",
    "print(f\"基础模型: {BASE_MODEL}\")\n",
    "print(f\"LoRA模型: {LORA_MODEL if LORA_MODEL else '未指定（仅使用基础模型）'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 定义辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(base_model_name, lora_model_path=None):\n",
    "    \"\"\"\n",
    "    加载模型和tokenizer\n",
    "    \n",
    "    Args:\n",
    "        base_model_name: 基础模型名称\n",
    "        lora_model_path: LoRA模型路径（如果为None则只加载基础模型）\n",
    "    \"\"\"\n",
    "    print(f\"加载基础模型: {base_model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # 如果提供了LoRA模型路径，则加载LoRA权重\n",
    "    if lora_model_path:\n",
    "        print(f\"加载LoRA权重: {lora_model_path}\")\n",
    "        model = PeftModel.from_pretrained(model, lora_model_path)\n",
    "        model = model.merge_and_unload()  # 合并LoRA权重到基础模型\n",
    "    \n",
    "    model.eval()\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_info(email_content, model, tokenizer, max_new_tokens=512, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    从邮件中提取事件信息\n",
    "    \n",
    "    Args:\n",
    "        email_content: 邮件内容\n",
    "        model: 模型\n",
    "        tokenizer: tokenizer\n",
    "        max_new_tokens: 最大生成token数\n",
    "        temperature: 温度参数\n",
    "        top_p: top_p采样参数\n",
    "    \n",
    "    Returns:\n",
    "        response: 提取的事件信息\n",
    "        inference_time: 推理时间（秒）\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 构建对话消息\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"你是一个专业的邮件事件信息提取助手。\"},\n",
    "        {\"role\": \"user\", \"content\": f\"请从以下邮件中提取事件信息，包括标题、时间、地点、参与者等关键信息，以JSON格式输出。\\n\\n邮件内容：\\n{email_content}\"}\n",
    "    ]\n",
    "    \n",
    "    # 使用chat template格式化\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # 生成\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # 解码输出\n",
    "    response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    \n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    return response, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(base_output: str, finetuned_output: str, ground_truth: str = None) -> Dict:\n",
    "    \"\"\"\n",
    "    对比基础模型和微调模型的输出\n",
    "    \n",
    "    Args:\n",
    "        base_output: 基础模型的输出\n",
    "        finetuned_output: 微调模型的输出\n",
    "        ground_truth: 真实标注（可选）\n",
    "    \n",
    "    Returns:\n",
    "        对比结果字典\n",
    "    \"\"\"\n",
    "    result = {\n",
    "        'base_valid_json': False,\n",
    "        'finetuned_valid_json': False,\n",
    "        'base_parsed': None,\n",
    "        'finetuned_parsed': None,\n",
    "        'ground_truth_parsed': None,\n",
    "        'differences': []\n",
    "    }\n",
    "    \n",
    "    # 解析基础模型输出\n",
    "    try:\n",
    "        result['base_parsed'] = json.loads(base_output)\n",
    "        result['base_valid_json'] = True\n",
    "    except json.JSONDecodeError:\n",
    "        result['base_parsed'] = None\n",
    "    \n",
    "    # 解析微调模型输出\n",
    "    try:\n",
    "        result['finetuned_parsed'] = json.loads(finetuned_output)\n",
    "        result['finetuned_valid_json'] = True\n",
    "    except json.JSONDecodeError:\n",
    "        result['finetuned_parsed'] = None\n",
    "    \n",
    "    # 解析真实标注\n",
    "    if ground_truth:\n",
    "        try:\n",
    "            result['ground_truth_parsed'] = json.loads(ground_truth)\n",
    "        except json.JSONDecodeError:\n",
    "            result['ground_truth_parsed'] = None\n",
    "    \n",
    "    # 如果两个模型都成功解析，对比字段差异\n",
    "    if result['base_parsed'] and result['finetuned_parsed']:\n",
    "        base_dict = result['base_parsed']\n",
    "        ft_dict = result['finetuned_parsed']\n",
    "        \n",
    "        all_keys = set(base_dict.keys()) | set(ft_dict.keys())\n",
    "        for key in all_keys:\n",
    "            base_val = base_dict.get(key, '(缺失)')\n",
    "            ft_val = ft_dict.get(key, '(缺失)')\n",
    "            \n",
    "            if base_val != ft_val:\n",
    "                diff_item = {\n",
    "                    'field': key,\n",
    "                    'base_value': base_val,\n",
    "                    'finetuned_value': ft_val\n",
    "                }\n",
    "                \n",
    "                # 如果有真实标注，添加对比\n",
    "                if result['ground_truth_parsed'] and key in result['ground_truth_parsed']:\n",
    "                    diff_item['ground_truth'] = result['ground_truth_parsed'][key]\n",
    "                    diff_item['base_correct'] = base_val == result['ground_truth_parsed'][key]\n",
    "                    diff_item['finetuned_correct'] = ft_val == result['ground_truth_parsed'][key]\n",
    "                \n",
    "                result['differences'].append(diff_item)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison(comparison: Dict, base_time: float, ft_time: float):\n",
    "    \"\"\"\n",
    "    打印对比结果\n",
    "    \n",
    "    Args:\n",
    "        comparison: 对比结果字典\n",
    "        base_time: 基础模型推理时间\n",
    "        ft_time: 微调模型推理时间\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"对比结果\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # JSON格式正确性\n",
    "    print(f\"\\nJSON格式:\")\n",
    "    print(f\"  基础模型: {'✓ 正确' if comparison['base_valid_json'] else '✗ 错误'}\")\n",
    "    print(f\"  微调模型: {'✓ 正确' if comparison['finetuned_valid_json'] else '✗ 错误'}\")\n",
    "    \n",
    "    # 推理时间\n",
    "    print(f\"\\n推理时间:\")\n",
    "    print(f\"  基础模型: {base_time:.2f}秒\")\n",
    "    print(f\"  微调模型: {ft_time:.2f}秒\")\n",
    "    print(f\"  速度变化: {((ft_time - base_time) / base_time * 100):+.1f}%\")\n",
    "    \n",
    "    # 字段差异\n",
    "    if comparison['differences']:\n",
    "        print(f\"\\n字段差异 ({len(comparison['differences'])}个):\")\n",
    "        print(\"-\" * 80)\n",
    "        for diff in comparison['differences']:\n",
    "            print(f\"\\n  字段: {diff['field']}\")\n",
    "            print(f\"    基础模型: {diff['base_value']}\")\n",
    "            print(f\"    微调模型: {diff['finetuned_value']}\")\n",
    "            \n",
    "            if 'ground_truth' in diff:\n",
    "                print(f\"    真实标注: {diff['ground_truth']}\")\n",
    "                print(f\"    基础模型准确: {'✓' if diff['base_correct'] else '✗'}\")\n",
    "                print(f\"    微调模型准确: {'✓' if diff['finetuned_correct'] else '✗'}\")\n",
    "    else:\n",
    "        print(\"\\n字段差异: 无差异（输出相同）\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 加载模型\n",
    "\n",
    "根据上面的配置加载模型（这个步骤会花费一些时间）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "开始加载模型...\n",
      "==================================================\n",
      "加载基础模型: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载LoRA权重: /macroverse/public/dingsz/infer_optim/qwen_finetune/outputs/lora_model/final_model\n",
      "\n",
      "✓ 模型加载完成！\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"开始加载模型...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "model, tokenizer = load_model(BASE_MODEL, LORA_MODEL)\n",
    "\n",
    "print(\"\\n✓ 模型加载完成！\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 单个样本测试\n",
    "\n",
    "使用一个示例邮件进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试邮件：\n",
      "主题：项目评审会议\n",
      "发件人：项目经理 张三\n",
      "收件人：开发团队\n",
      "\n",
      "各位同事，\n",
      "\n",
      "定于本周五（12月29日）下午3点在会议室B召开项目中期评审会议。请技术负责人和架构师务必参加，并准备项目进展汇报材料。\n",
      "\n",
      "谢谢！\n",
      "张三\n",
      "\n",
      "==================================================\n",
      "提取中...\n",
      "\n",
      "提取结果：\n",
      "{\n",
      "  \"event_type\": \"会议\",\n",
      "  \"title\": \"项目中期评审会议\",\n",
      "  \"time\": \"本周五（12月29日）下午3点\",\n",
      "  \"location\": \"会议室B\",\n",
      "  \"participants\": [\"技术负责人\", \"架构师\"],\n",
      "  \"organizer\": \"项目经理 张三\"\n",
      "}\n",
      "\n",
      "推理时间: 3.29秒\n",
      "✓ JSON格式正确\n",
      "\n",
      "格式化的JSON：\n",
      "{\n",
      "  \"event_type\": \"会议\",\n",
      "  \"title\": \"项目中期评审会议\",\n",
      "  \"time\": \"本周五（12月29日）下午3点\",\n",
      "  \"location\": \"会议室B\",\n",
      "  \"participants\": [\n",
      "    \"技术负责人\",\n",
      "    \"架构师\"\n",
      "  ],\n",
      "  \"organizer\": \"项目经理 张三\"\n",
      "}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 定义测试邮件\n",
    "test_email = \"\"\"主题：项目评审会议\n",
    "发件人：项目经理 张三\n",
    "收件人：开发团队\n",
    "\n",
    "各位同事，\n",
    "\n",
    "定于本周五（12月29日）下午3点在会议室B召开项目中期评审会议。请技术负责人和架构师务必参加，并准备项目进展汇报材料。\n",
    "\n",
    "谢谢！\n",
    "张三\"\"\"\n",
    "\n",
    "print(\"测试邮件：\")\n",
    "print(test_email)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"提取中...\")\n",
    "\n",
    "result, inference_time = extract_event_info(\n",
    "    test_email, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    MAX_NEW_TOKENS,\n",
    "    TEMPERATURE,\n",
    "    TOP_P\n",
    ")\n",
    "\n",
    "print(f\"\\n提取结果：\\n{result}\")\n",
    "print(f\"\\n推理时间: {inference_time:.2f}秒\")\n",
    "\n",
    "# 验证JSON格式\n",
    "try:\n",
    "    parsed_result = json.loads(result)\n",
    "    print(\"✓ JSON格式正确\")\n",
    "    print(\"\\n格式化的JSON：\")\n",
    "    print(json.dumps(parsed_result, ensure_ascii=False, indent=2))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"✗ JSON格式错误: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 自定义邮件测试\n",
    "\n",
    "在这里输入你自己的邮件内容进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改这里的邮件内容\n",
    "custom_email = \"\"\"主题：团建活动通知\n",
    "发件人：HR部门\n",
    "收件人：全体员工\n",
    "\n",
    "大家好，\n",
    "\n",
    "公司将于下周六（1月15日）上午10点组织团建活动，地点在市郊的绿野公园。请大家准时参加，穿着运动服装。\n",
    "\n",
    "活动内容包括户外拓展训练和午餐聚会。\n",
    "\n",
    "期待大家的参与！\n",
    "HR部门\"\"\"\n",
    "\n",
    "print(\"自定义邮件：\")\n",
    "print(custom_email)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"提取中...\")\n",
    "\n",
    "result, inference_time = extract_event_info(\n",
    "    custom_email, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    MAX_NEW_TOKENS,\n",
    "    TEMPERATURE,\n",
    "    TOP_P\n",
    ")\n",
    "\n",
    "print(f\"\\n提取结果：\\n{result}\")\n",
    "print(f\"\\n推理时间: {inference_time:.2f}秒\")\n",
    "\n",
    "# 验证JSON格式\n",
    "try:\n",
    "    parsed_result = json.loads(result)\n",
    "    print(\"✓ JSON格式正确\")\n",
    "    print(\"\\n格式化的JSON：\")\n",
    "    print(json.dumps(parsed_result, ensure_ascii=False, indent=2))\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"✗ JSON格式错误: {e}\")\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 批量测试（可选）\n",
    "\n",
    "如果你有测试数据文件，可以在这里进行批量测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置测试数据文件路径\n",
    "test_file_path = TEST_FILE  # 修改为你的测试文件路径，例如: \"../data/test.jsonl\"\n",
    "\n",
    "if test_file_path and os.path.exists(test_file_path):\n",
    "    print(f\"从文件读取测试邮件: {test_file_path}\")\n",
    "    with open(test_file_path, 'r', encoding='utf-8') as f:\n",
    "        test_data = [json.loads(line) for line in f]\n",
    "    \n",
    "    print(f\"共 {len(test_data)} 条测试样本\\n\")\n",
    "    \n",
    "    total_inference_time = 0\n",
    "    valid_json_count = 0\n",
    "    \n",
    "    # 限制测试样本数量\n",
    "    samples_to_test = test_data[:MAX_SAMPLES] if MAX_SAMPLES else test_data\n",
    "    \n",
    "    for i, item in enumerate(samples_to_test):\n",
    "        print(f\"\\n{'=' * 50}\")\n",
    "        print(f\"测试样本 {i + 1}/{len(samples_to_test)}\")\n",
    "        print(f\"{'=' * 50}\")\n",
    "        \n",
    "        # 支持不同数据格式\n",
    "        if 'messages' in item:\n",
    "            email_content = item['messages'][1]['content'].split('邮件内容：\\n')[-1]\n",
    "            expected_output = item['messages'][2]['content']\n",
    "        elif 'input' in item:\n",
    "            email_content = item['input']\n",
    "            expected_output = item.get('output')\n",
    "        else:\n",
    "            print(\"⚠️  数据格式不支持，跳过\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n输入邮件：\\n{email_content[:200]}...\" if len(email_content) > 200 else f\"\\n输入邮件：\\n{email_content}\")\n",
    "        \n",
    "        result, inference_time = extract_event_info(\n",
    "            email_content, \n",
    "            model, \n",
    "            tokenizer, \n",
    "            MAX_NEW_TOKENS,\n",
    "            TEMPERATURE,\n",
    "            TOP_P\n",
    "        )\n",
    "        total_inference_time += inference_time\n",
    "        \n",
    "        print(f\"\\n模型输出：\\n{result}\")\n",
    "        print(f\"\\n推理时间: {inference_time:.2f}秒\")\n",
    "        \n",
    "        # 验证JSON格式\n",
    "        try:\n",
    "            json.loads(result)\n",
    "            valid_json_count += 1\n",
    "            print(\"✓ JSON格式正确\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"✗ JSON格式错误\")\n",
    "    \n",
    "    # 统计信息\n",
    "    avg_time = total_inference_time / len(samples_to_test)\n",
    "    json_accuracy = valid_json_count / len(samples_to_test) * 100\n",
    "    \n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(\"推理统计:\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    print(f\"平均推理时间: {avg_time:.2f}秒/样本\")\n",
    "    print(f\"JSON格式正确率: {json_accuracy:.1f}%\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "else:\n",
    "    print(f\"测试文件不存在或未指定: {test_file_path}\")\n",
    "    print(\"请在上面的配置中设置 TEST_FILE 变量为有效的测试数据文件路径\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 对比测试（可选）\n",
    "\n",
    "如果你想对比基础模型和微调模型的效果，可以运行这个部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "对比模式：同时加载基础模型和微调模型\n",
      "基础模型: Qwen/Qwen2.5-7B-Instruct\n",
      "微调模型: /macroverse/public/dingsz/infer_optim/qwen_finetune/outputs/lora_model/final_model\n",
      "\n",
      "[1/2] 加载基础模型...\n",
      "加载基础模型: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/2] 加载微调模型...\n",
      "加载基础模型: Qwen/Qwen2.5-7B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载LoRA权重: /macroverse/public/dingsz/infer_optim/qwen_finetune/outputs/lora_model/final_model\n",
      "\n",
      "✓ 两个模型加载完成！\n",
      "==================================================\n",
      "\n",
      "测试邮件:\n",
      "主题：项目评审会议\n",
      "发件人：项目经理 张三\n",
      "收件人：开发团队\n",
      "\n",
      "各位同事，\n",
      "\n",
      "定于本周五（12月29日）下午3点在会议室B召开项目中期评审会议。请技术负责人和架构师务必参加，并准备项目进展汇报材料。\n",
      "\n",
      "谢谢！\n",
      "张三\n",
      "\n",
      "================================================================================\n",
      "[1/2] 基础模型推理中...\n",
      "[2/2] 微调模型推理中...\n",
      "\n",
      "基础模型输出:\n",
      "```json\n",
      "{\n",
      "  \"标题\": \"项目评审会议\",\n",
      "  \"时间\": \"12月29日 下午3点\",\n",
      "  \"地点\": \"会议室B\",\n",
      "  \"参与者\": [\n",
      "    {\n",
      "      \"角色\": \"技术负责人\",\n",
      "      \"备注\": \"需准备项目进展汇报材料\"\n",
      "    },\n",
      "    {\n",
      "      \"角色\": \"架构师\",\n",
      "      \"备注\": \"需准备项目进展汇报材料\"\n",
      "    }\n",
      "  ],\n",
      "  \"发起人\": \"项目经理 张三\",\n",
      "  \"接收人\": \"开发团队\"\n",
      "}\n",
      "```\n",
      "\n",
      "微调模型输出:\n",
      "{\n",
      "  \"event_type\": \"会议\",\n",
      "  \"title\": \"项目中期评审会议\",\n",
      "  \"time\": \"本周五（12月29日）下午3点\",\n",
      "  \"location\": \"会议室B\",\n",
      "  \"participants\": [\"开发团队的技术负责人\", \"架构师\"],\n",
      "  \"organizer\": \"项目经理 张三\"\n",
      "}\n",
      "\n",
      "================================================================================\n",
      "对比结果\n",
      "================================================================================\n",
      "\n",
      "JSON格式:\n",
      "  基础模型: ✗ 错误\n",
      "  微调模型: ✓ 正确\n",
      "\n",
      "推理时间:\n",
      "  基础模型: 3.27秒\n",
      "  微调模型: 2.10秒\n",
      "  速度变化: -35.5%\n",
      "\n",
      "字段差异: 无差异（输出相同）\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 对比模式需要重新加载两个模型\n",
    "# 设置对比模式的配置\n",
    "COMPARE_MODE = True  # 设置为 True 启用对比模式\n",
    "LORA_MODEL_FOR_COMPARE = \"/macroverse/public/dingsz/infer_optim/qwen_finetune/outputs/lora_model/final_model\"  # 设置你的LoRA模型路径\n",
    "\n",
    "if COMPARE_MODE and LORA_MODEL_FOR_COMPARE:\n",
    "    print(\"\\n对比模式：同时加载基础模型和微调模型\")\n",
    "    print(f\"基础模型: {BASE_MODEL}\")\n",
    "    print(f\"微调模型: {LORA_MODEL_FOR_COMPARE}\")\n",
    "    \n",
    "    # 加载基础模型\n",
    "    print(\"\\n[1/2] 加载基础模型...\")\n",
    "    base_model, base_tokenizer = load_model(BASE_MODEL, lora_model_path=None)\n",
    "    \n",
    "    # 加载微调模型\n",
    "    print(\"\\n[2/2] 加载微调模型...\")\n",
    "    ft_model, ft_tokenizer = load_model(BASE_MODEL, LORA_MODEL_FOR_COMPARE)\n",
    "    \n",
    "    print(\"\\n✓ 两个模型加载完成！\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 使用测试邮件进行对比\n",
    "    email_content = test_email\n",
    "    \n",
    "    print(f\"\\n测试邮件:\\n{email_content}\")\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "    # 基础模型推理\n",
    "    print(\"[1/2] 基础模型推理中...\")\n",
    "    base_result, base_time = extract_event_info(\n",
    "        email_content, \n",
    "        base_model, \n",
    "        base_tokenizer, \n",
    "        MAX_NEW_TOKENS,\n",
    "        TEMPERATURE,\n",
    "        TOP_P\n",
    "    )\n",
    "    \n",
    "    # 微调模型推理\n",
    "    print(\"[2/2] 微调模型推理中...\")\n",
    "    ft_result, ft_time = extract_event_info(\n",
    "        email_content, \n",
    "        ft_model, \n",
    "        ft_tokenizer, \n",
    "        MAX_NEW_TOKENS,\n",
    "        TEMPERATURE,\n",
    "        TOP_P\n",
    "    )\n",
    "    \n",
    "    # 显示原始输出\n",
    "    print(f\"\\n基础模型输出:\\n{base_result}\")\n",
    "    print(f\"\\n微调模型输出:\\n{ft_result}\")\n",
    "    \n",
    "    # 对比输出\n",
    "    comparison = compare_outputs(base_result, ft_result)\n",
    "    print_comparison(comparison, base_time, ft_time)\n",
    "    \n",
    "elif COMPARE_MODE:\n",
    "    print(\"⚠️  对比模式需要设置 LORA_MODEL_FOR_COMPARE\")\n",
    "else:\n",
    "    print(\"对比模式未启用。如需启用，请设置 COMPARE_MODE = True 和 LORA_MODEL_FOR_COMPARE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 释放显存（可选）\n",
    "\n",
    "如果需要释放GPU显存，可以运行这个单元格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 显存已释放\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# 删除模型对象\n",
    "if 'model' in globals():\n",
    "    del model\n",
    "if 'tokenizer' in globals():\n",
    "    del tokenizer\n",
    "if 'base_model' in globals():\n",
    "    del base_model\n",
    "if 'base_tokenizer' in globals():\n",
    "    del base_tokenizer\n",
    "if 'ft_model' in globals():\n",
    "    del ft_model\n",
    "if 'ft_tokenizer' in globals():\n",
    "    del ft_tokenizer\n",
    "\n",
    "# 清理缓存\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✓ 显存已释放\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
