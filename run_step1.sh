#!/bin/bash

# LoRAå¾®è°ƒå®Œæ•´æµç¨‹è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•ï¼ˆä½¿ç”¨å·²ä¸‹è½½çš„æ¨¡å‹ï¼‰
export HF_HOME=/macroverse/public/database/huggingface/hub
export HF_DATASETS_CACHE=/macroverse/public/database/huggingface/datasets

echo "======================================================"
echo "ç¬¬ä¸€æ­¥ï¼šLoRAå¾®è°ƒå®Œæ•´æµç¨‹"
echo "======================================================"
echo ""
echo "ç¯å¢ƒå˜é‡è®¾ç½®ï¼š"
echo "  HF_HOME=$HF_HOME"
echo ""

# æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
echo ""
echo "æ­¥éª¤ 1/4: æ£€æŸ¥è®­ç»ƒæ•°æ®"
echo "------------------------------------------------------"
if [ ! -f "data/processed/train.jsonl" ] || [ ! -f "data/processed/valid.jsonl" ] || [ ! -f "data/processed/test.jsonl" ]; then
    echo "âš ï¸  è®­ç»ƒæ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®é¢„å¤„ç†è„šæœ¬ï¼š"
    # echo "  1. uv run python scripts/01_process_enron_csv.py"
    # echo "  2. uv run python scripts/02_generate_annotations.py"
    # echo "  3. uv run python scripts/03_split_dataset.py"
    exit 1
else
    echo "âœ“ è®­ç»ƒæ•°æ®å­˜åœ¨"
    echo "  - train.jsonl: $(wc -l < data/processed/train.jsonl) æ¡ï¼ˆè®­ç»ƒé›†ï¼‰"
    echo "  - valid.jsonl: $(wc -l < data/processed/valid.jsonl) æ¡ï¼ˆéªŒè¯é›†ï¼Œç”¨äºæ—©åœï¼‰"
    echo "  - test.jsonl: $(wc -l < data/processed/test.jsonl) æ¡ï¼ˆæµ‹è¯•é›†ï¼Œç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰"
fi


# è®­ç»ƒæ¨¡å‹
echo ""
echo "æ­¥éª¤ 2/4: å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒï¼ˆå¯ç”¨æ—©åœï¼‰"
echo "------------------------------------------------------"
uv run python scripts/train_lora.py \
    --model_name Qwen/Qwen2.5-7B-Instruct \
    --train_data data/processed/train.jsonl \
    --eval_data data/processed/valid.jsonl \
    --output_dir outputs/lora_model \
    --num_epochs 10 \
    --batch_size 1 \
    --gradient_accumulation_steps 8 \
    --learning_rate 2e-4 \
    --lora_r 16 \
    --lora_alpha 32 \
    --early_stopping \
    --early_stopping_patience 5 \
    --metric_for_best_model eval_loss

# è¯„ä¼°æ¨¡å‹å¯¹æ¯”
echo ""
echo "æ­¥éª¤ 3/4: è¯„ä¼°æ¨¡å‹æ•ˆæœï¼ˆå¯¹æ¯”åŸå§‹æ¨¡å‹ vs å¾®è°ƒæ¨¡å‹ï¼‰"
echo "------------------------------------------------------"
uv run python scripts/evaluate_models.py \
    --base_model Qwen/Qwen2.5-7B-Instruct \
    --lora_model outputs/lora_model/final_model \
    --test_file data/processed/test.jsonl \
    --max_samples None  \
    --output_file outputs/evaluation_results.json

# # æµ‹è¯•æ¨ç†
# echo ""
# echo "æ­¥éª¤ 4/4: æµ‹è¯•æ¨ç†æ•ˆæœ"
# echo "------------------------------------------------------"
# uv run python scripts/inference.py \
#     --lora_model outputs/lora_model/final_model \
#     --test_file data/processed/test.jsonl \
#     --max_samples 5

echo ""
echo "======================================================"
echo "LoRAå¾®è°ƒæµç¨‹å®Œæˆï¼"
echo "======================================================"
echo ""
echo "ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦ï¼š"
echo "------------------------------------------------------"
if [ -f "outputs/evaluation_results.json" ]; then
    echo "è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: outputs/evaluation_results.json"
    python3 -c "
import json
with open('outputs/evaluation_results.json', 'r') as f:
    data = json.load(f)
    base = data['base_metrics']
    ft = data['finetuned_metrics']
    imp = data['improvements']

    print(f\"\\næŒ‡æ ‡å¯¹æ¯”ï¼š\")
    print(f\"  JSONæ ¼å¼æ­£ç¡®ç‡: {base['json_format_accuracy']:.1f}% â†’ {ft['json_format_accuracy']:.1f}% (æå‡ {imp['json_format_accuracy']:+.1f}%)\")
    print(f\"  å­—æ®µå®Œæ•´æ€§: {base['field_completeness']:.1f}% â†’ {ft['field_completeness']:.1f}% (æå‡ {imp['field_completeness']:+.1f}%)\")
    print(f\"  å­—æ®µå‡†ç¡®æ€§: {base['field_accuracy']:.1f}% â†’ {ft['field_accuracy']:.1f}% (æå‡ {imp['field_accuracy']:+.1f}%)\")
    print(f\"  å®Œå…¨åŒ¹é…ç‡: {base['exact_match_rate']:.1f}% â†’ {ft['exact_match_rate']:.1f}% (æå‡ {imp['exact_match_rate']:+.1f}%)\")
"
fi
# echo ""
# echo "ğŸ“‚ è¾“å‡ºæ–‡ä»¶ï¼š"
# echo "  - æ¨¡å‹: outputs/lora_model/final_model"
# echo "  - è®­ç»ƒæ—¥å¿—: outputs/lora_model/logs"
# echo "  - è¯„ä¼°ç»“æœ: outputs/evaluation_results.json"
# echo ""
# echo "ğŸ” ä¸‹ä¸€æ­¥å¯ä»¥ï¼š"
# echo "  1. æŸ¥çœ‹è®­ç»ƒæ›²çº¿: swanlab watch (æˆ–è®¿é—® SwanLab Web UI)"
# echo "  2. æŸ¥çœ‹è¯¦ç»†è¯„ä¼°: cat outputs/evaluation_results.json | jq"
# echo "  3. äº¤äº’å¼æµ‹è¯•: uv run python scripts/inference.py --lora_model outputs/lora_model/final_model --interactive"
# echo "  4. å®Œæ•´æµ‹è¯•é›†: uv run python scripts/inference.py --lora_model outputs/lora_model/final_model --test_file data/processed/test.jsonl"
# echo ""
