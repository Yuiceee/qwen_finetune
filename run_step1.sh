#!/bin/bash

# LoRAå¾®è°ƒå®Œæ•´æµç¨‹è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# è®¾ç½®HuggingFaceç¼“å­˜ç›®å½•ï¼ˆä½¿ç”¨å·²ä¸‹è½½çš„æ¨¡å‹ï¼‰
export HF_HOME=/macroverse/public/database/huggingface/hub
export HF_DATASETS_CACHE=/macroverse/public/database/huggingface/datasets

echo "======================================================"
echo "ç¬¬ä¸€æ­¥ï¼šLoRAå¾®è°ƒå®Œæ•´æµç¨‹"
echo "======================================================"
echo ""
echo "ç¯å¢ƒå˜é‡è®¾ç½®ï¼š"
echo "  HF_HOME=$HF_HOME"
echo ""

# æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
echo ""
echo "æ­¥éª¤ 1/4: æ£€æŸ¥è®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨æ¸…ç†å’Œæ ‡å‡†åŒ–åçš„æ•°æ®ï¼‰"
echo "------------------------------------------------------"
if [ ! -f "data/reviewed/train.jsonl" ] || [ ! -f "data/reviewed/valid.jsonl" ] || [ ! -f "data/reviewed/test.jsonl" ]; then
    echo "âš ï¸  æ ‡å‡†åŒ–æ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œæ•°æ®æ¸…ç†è„šæœ¬ï¼š"
    echo "  1. uv run python scripts/clean_and_standardize_data.py"
    echo "  2. uv run python scripts/standardize_time_fields_v2.py --execute --min_confidence=medium"
    exit 1
else
    echo "âœ“ æ ‡å‡†åŒ–è®­ç»ƒæ•°æ®å­˜åœ¨ (V2 - æ¸…ç†å)"
    echo "  - train.jsonl: $(wc -l < data/reviewed/train.jsonl) æ¡ï¼ˆè®­ç»ƒé›†ï¼‰"
    echo "  - valid.jsonl: $(wc -l < data/reviewed/valid.jsonl) æ¡ï¼ˆéªŒè¯é›†ï¼Œç”¨äºæ—©åœï¼‰"
    echo "  - test.jsonl: $(wc -l < data/reviewed/test.jsonl) æ¡ï¼ˆæµ‹è¯•é›†ï¼Œç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰"
    echo ""
    echo "  æ•°æ®è´¨é‡æ”¹è¿›ï¼š"
    echo "    âœ“ ç»Ÿä¸€ä¸ºæ ¸å¿ƒ6å­—æ®µ (event_type, title, time, location, participants, organizer)"
    echo "    âœ“ ç§»é™¤JSONæ ¼å¼é”™è¯¯"
    echo "    âœ“ timeå­—æ®µæ ‡å‡†åŒ–ä¸º YYYY-MM-DD æˆ– YYYY-MM-DD HH:MM"
fi


# è®­ç»ƒæ¨¡å‹
echo ""
echo "æ­¥éª¤ 2/4: å¼€å§‹LoRAå¾®è°ƒè®­ç»ƒ V2ï¼ˆä½¿ç”¨ä¼˜åŒ–å‚æ•°ï¼‰"
echo "------------------------------------------------------"
echo "è®­ç»ƒé…ç½®ï¼š"
echo "  - æ•°æ®: data/reviewed/ "
echo "  - å­¦ä¹ ç‡: 1e-4 (é™ä½ä»¥æé«˜ç¨³å®šæ€§)"
echo "  - Warmup: 10% (æ›´å¥½çš„æ”¶æ•›)"
echo "  - Epochs: 5 (å¸¦early stopping)"
echo "  - æ—©åœè€å¿ƒ: 5ä¸ªè¯„ä¼°æ­¥éª¤"
echo ""
uv run python scripts/train_lora.py \
    --model_name Qwen/Qwen2.5-7B-Instruct \
    --train_data data/reviewed/train.jsonl \
    --eval_data data/reviewed/valid.jsonl \
    --output_dir outputs/lora_model_v2 \
    --num_epochs 5 \
    --batch_size 1 \
    --gradient_accumulation_steps 8 \
    --learning_rate 1e-4 \
    --warmup_ratio 0.1 \
    --lora_r 16 \
    --lora_alpha 32 \
    --early_stopping \
    --early_stopping_patience 5 \
    --metric_for_best_model eval_loss

# è¯„ä¼°æ¨¡å‹å¯¹æ¯”
echo ""
echo "æ­¥éª¤ 3/4: è¯„ä¼°æ¨¡å‹æ•ˆæœ V2ï¼ˆå¯¹æ¯”åŸå§‹æ¨¡å‹ vs å¾®è°ƒæ¨¡å‹ï¼‰"
echo "------------------------------------------------------"
uv run python scripts/evaluate_models.py \
    --base_model Qwen/Qwen2.5-7B-Instruct \
    --lora_model outputs/lora_model_v2/final_model \
    --test_file data/reviewed/test.jsonl \
    --output_file outputs/evaluation_results_v2.json

# # æµ‹è¯•æ¨ç†
# echo ""
# echo "æ­¥éª¤ 4/4: æµ‹è¯•æ¨ç†æ•ˆæœ"
# echo "------------------------------------------------------"
# uv run python scripts/inference.py \
#     --lora_model outputs/lora_model/final_model \
#     --test_file data/processed/test.jsonl \
#     --max_samples 5

echo ""
echo "======================================================"
echo "LoRAå¾®è°ƒæµç¨‹ V2 å®Œæˆï¼"
echo "======================================================"
echo ""
echo "ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦ (V2 - ä½¿ç”¨æ¸…ç†åæ•°æ®)ï¼š"
echo "------------------------------------------------------"
if [ -f "outputs/evaluation_results_v2.json" ]; then
    echo "è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: outputs/evaluation_results_v2.json"
    python3 -c "
import json
with open('outputs/evaluation_results_v2.json', 'r') as f:
    data = json.load(f)
    base = data['base_metrics']
    ft = data['finetuned_metrics']
    imp = data['improvements']

    print(f\"\\n=== æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯” ===\")
    print(f\"  JSONæ ¼å¼æ­£ç¡®ç‡: {base['json_format_accuracy']:.1f}% â†’ {ft['json_format_accuracy']:.1f}% (æå‡ {imp['json_format_accuracy']:+.1f}%)\")
    print(f\"  å¹³å‡å­—æ®µå‡†ç¡®ç‡: {base['average_field_accuracy']:.1f}% â†’ {ft['average_field_accuracy']:.1f}% (æå‡ {imp['average_field_accuracy']:+.1f}%)\")
    print(f\"  å®Œç¾æå–ç‡: {base['perfect_extraction_rate']:.1f}% â†’ {ft['perfect_extraction_rate']:.1f}% (æå‡ {imp['perfect_extraction_rate']:+.1f}%)\")
    print(f\"\\n=== åˆ†å­—æ®µå‡†ç¡®ç‡ ===\")
    print(f\"  äº‹ä»¶ç±»å‹: {base['event_type_accuracy']:.1f}% â†’ {ft['event_type_accuracy']:.1f}% ({imp['event_type_accuracy']:+.1f}%)\")
    print(f\"  æ ‡é¢˜: {base['title_accuracy']:.1f}% â†’ {ft['title_accuracy']:.1f}% ({imp['title_accuracy']:+.1f}%)\")
    print(f\"  æ—¶é—´: {base['time_accuracy']:.1f}% â†’ {ft['time_accuracy']:.1f}% ({imp['time_accuracy']:+.1f}%)\")
    print(f\"  åœ°ç‚¹: {base['location_accuracy']:.1f}% â†’ {ft['location_accuracy']:.1f}% ({imp['location_accuracy']:+.1f}%)\")
    print(f\"  å‚ä¸è€…: {base['participants_accuracy']:.1f}% â†’ {ft['participants_accuracy']:.1f}% ({imp['participants_accuracy']:+.1f}%)\")
    print(f\"  ç»„ç»‡è€…: {base['organizer_accuracy']:.1f}% â†’ {ft['organizer_accuracy']:.1f}% ({imp['organizer_accuracy']:+.1f}%)\")
"
fi
# echo ""
# echo "ğŸ“‚ è¾“å‡ºæ–‡ä»¶ï¼š"
# echo "  - æ¨¡å‹: outputs/lora_model/final_model"
# echo "  - è®­ç»ƒæ—¥å¿—: outputs/lora_model/logs"
# echo "  - è¯„ä¼°ç»“æœ: outputs/evaluation_results.json"
# echo ""
# echo "ğŸ” ä¸‹ä¸€æ­¥å¯ä»¥ï¼š"
# echo "  1. æŸ¥çœ‹è®­ç»ƒæ›²çº¿: swanlab watch (æˆ–è®¿é—® SwanLab Web UI)"
# echo "  2. æŸ¥çœ‹è¯¦ç»†è¯„ä¼°: cat outputs/evaluation_results.json | jq"
# echo "  3. äº¤äº’å¼æµ‹è¯•: uv run python scripts/inference.py --lora_model outputs/lora_model/final_model --interactive"
# echo "  4. å®Œæ•´æµ‹è¯•é›†: uv run python scripts/inference.py --lora_model outputs/lora_model/final_model --test_file data/processed/test.jsonl"
# echo ""
